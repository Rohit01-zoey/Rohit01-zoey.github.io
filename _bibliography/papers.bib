---
---

@InProceedings{10.1007/978-3-031-44201-8_21,
author="K. Vartak, Rohit
and Saraswat, Vivek
and Ganguly, Udayan",
editor="Iliadis, Lazaros
and Papaleonidas, Antonios
and Angelov, Plamen
and Jayne, Chrisina",
title="Robustness to Variability and Asymmetry of In-Memory On-Chip Training",
booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2023",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="249--257",
abstract="In-memory on-chip learning is crucial for low-power, in-field training capabilities at the edge. We demonstrate the robustness of on-chip back-propagation to hardware variability in terms of bit-cell transistor {\$}{\$}V{\_}T{\$}{\$}variability ({\$}{\$}2.5{\backslash}times {\$}{\$}more robust than off-chip training). We use perturbation schemes, asymmetry variations and variability-aware update schemes to identify the relative contribution of different on-chip operations: forward pass, backward pass and weight updates to Fashion-MNIST classification performance degradation with variations. It is revealed that variability during the weight update step is crucial while accuracy of backward pass or gradient calculation is not critical. We promote weight perturbation scheme over back-propagation as the choice for on-chip in-memory training with reduced points of failure and low cost of hardware.",
isbn="978-3-031-44201-8"
}

@misc{singhal2025fedsbsilverbulletextreme,
      title={Fed-SB: A Silver Bullet for Extreme Communication Efficiency and Performance in (Private) Federated LoRA Fine-Tuning}, 
      author={Raghav Singhal and Kaustubh Ponkshe and Rohit Vartak and Lav R. Varshney and Praneeth Vepakomma},
      year={2025},
      eprint={2502.15436},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      abstract="Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuning foundation models. However, federated fine-tuning using LoRA is challenging due to suboptimal updates arising from traditional federated averaging of individual adapters. Existing solutions either incur prohibitively high communication cost that scales linearly with the number of clients or suffer from performance degradation due to limited expressivity. We introduce Federated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning of LLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SB optimally aligns the optimization trajectory with the ideal low-rank full fine-tuning projection by learning a small square matrix (R) between adapters B and A, keeping other components fixed. Direct averaging of R guarantees exact updates, substantially reducing communication cost, which remains independent of the number of clients, and enables scalability. Fed-SB achieves state-of-the-art performance across commonsense reasoning, arithmetic reasoning, and language inference tasks while reducing communication costs by up to 230x. In private settings, Fed-SB further improves performance by (1) reducing trainable parameters, thereby lowering the noise required for differential privacy and (2) avoiding noise amplification introduced by other methods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoff between communication and performance, offering an efficient and scalable solution for both private and non-private federated fine-tuning.",
      url={https://arxiv.org/abs/2502.15436}, 
      code={https://github.com/CERT-Lab/fed-sb},
      preview={fed_sb_img.jpg}
}

@misc{singhal2025abbahighlyexpressivehadamard,
      title={ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models}, 
      author={Raghav Singhal and Kaustubh Ponkshe and Rohit Vartak and Praneeth Vepakomma},
      year={2025},
      eprint={2505.14238},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.14238}, 

}

@misc{paruchuri2025whatsupdocanalyzing,
      title={"What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets}, 
      author={Akshay Paruchuri and Maryam Aziz and Rohit Vartak and Ayman Ali and Best Uchehara and Xin Liu and Ishan Chatterjee and Monica Agrawal},
      year={2025},
      eprint={2506.21532},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.21532}, 
}