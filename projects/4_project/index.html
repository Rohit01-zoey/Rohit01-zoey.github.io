<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>ABBA | Rohit K. Vartak</title> <meta name="author" content="Rohit K. Vartak"> <meta name="description" content="Hadamard style update for LoRA"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rohit01-zoey.github.io/projects/4_project/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "ABBA",
      "description": "Hadamard style update for LoRA",
      "published": "July 30, 2025",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Rohit¬†</span>K.¬†Vartak</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>ABBA</h1> <p>Hadamard style update for LoRA</p> </d-title><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#overview">Overview</a></div> <div><a href="#methods">Methods</a></div> <ul> <li><a href="#full-fine-tuning-and-other-lora-methods">Full fine-tuning and other LoRA methods</a></li> <li><a href="#abba">ABBA</a></li> </ul> <div><a href="#practical-stuff-while-implementing-abba">Practical stuff while implementing ABBA</a></div> <ul> <li><a href="#implementing-abba-efficiently">Implementing ABBA efficiently</a></li> <li><a href="#abba-space-is-not-the-lora-space">ABBA space is not the LoRA space</a></li> </ul> <div><a href="#conclusion">Conclusion</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <h2 id="overview">Overview</h2> <p>We introduce <strong>ABBA</strong>, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget. We formally analyze ABBA‚Äôs expressive capacity and validate its advantages through matrix reconstruction experiments. Empirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models.</p> <h2 id="methods">Methods</h2> <h3 id="full-fine-tuning-and-other-lora-methods">Full fine-tuning and other LoRA methods</h3> <p><strong>Full fine-tuning</strong>: Given a pre-trained weight matrix $W_0 \in \mathbb{R}^{m \times n}$, full FT updates all parameters via $W = W_0 + \Delta W$, introducing $m \times n$ trainable parameters per layer. This quickly becomes impractical due to the high memory and compute overhead.</p> <p><strong>LoRA</strong><d-cite key="lora"></d-cite>: LoRA mitigates this by modeling the update as a low-rank decomposition: $\Delta W = sBA$, where $B \in \mathbb{R}^{m \times r}$, $A \in \mathbb{R}^{r \times n}$, and $s$ is a scaling factor. This reduces the number of trainable parameters to $r(m + n)$, with $r \ll \min(m, n)$. LoRA can represent any update of rank at most $r$, but cannot express higher-rank updates. Moreover, the projected gradient onto the weight space is also low-rank. While effective for simpler tasks, this limitation becomes significant in settings requiring high-rank updates or gradients <d-cite key="LoRA-Pro"></d-cite><d-cite key="ponkshe2025initializationusingupdateapproximation"></d-cite>.</p> <p><strong>HiRA (Hadamard High-Rank Adaptation)</strong><d-cite key="huang2025hira"></d-cite>: HiRA improves upon LoRA by applying a Hadamard product between the pre-trained weight $W_0$ and a low-rank update $BA$, enabling effective update ranks up to $r_0 r$ (see Thm (1) - a well known result). This improves on LoRA‚Äôs expressivity limits. However, because the update is element-wise tied to $W_0$, HiRA‚Äôs expressiveness is constrained to its support, which may hinder generalization‚Äîespecially out-of-domain.</p> <div class="theorem"> <strong>Theorem 1.</strong> Suppose $W_1$ and $W_2$ are matrices of rank $r_1$ and $r_2$ respectively. Then $$ \operatorname{rank}(W_1 \odot W_2) \leq r_1 \cdot r_2 $$ </div> <p><em>(By the way, the Hadamard product is surprisingly interesting ‚Äî not as trivial as I initially thought. I wrote a bit more about it <a href="../hadamard-musings">here</a> if you‚Äôre curious about some of the side paths I explored while working on this idea.)</em></p> <h3 id="abba">ABBA</h3> <p>We asked a natural question: <em>what if</em> we no longer kept $W_0$ frozen? <em>What if</em> we made it fully trainable? Unfortunately, this brings us right back to the original challenge of full fine-tuning üòû ‚Äî expensive and inefficient. So instead, we apply the classic LoRA trick: decompose the second ‚Äúfrozen‚Äù adaptor into a low-rank form and make <strong>that</strong> trainable. This gives rise to the following update:</p> <p>\begin{equation} \Delta W = s(B_1 A_1) \odot (B_2 A_2), \end{equation} where $B_1 \in \mathbb{R}^{m \times r_1},\; A_1 \in \mathbb{R}^{r_1 \times n}$ and<br> $B_2 \in \mathbb{R}^{m \times r_2},\; A_2 \in \mathbb{R}^{r_2 \times n}$, with $r_1, r_2 \ll \min(m, n)$ and $s$ is a scaling factor for stability.</p> <p><strong># of Parameters</strong> $= (r_1 + r_2)(m+n)$</p> <p><strong>Rank we can express</strong> $= r_1 r_2$ (using Thm (1) again üëÄ)</p> <p>Thus, setting $r_1=r_2 = \frac{r}{2}$ not only do we reach the same parameter budget as our baselines but also can represent matrices till rank $\frac{r^2}{4}$ (i.e maximizing the rank max)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;img
  src="/assets/img/abba/abba_new.png"
  class="img-fluid rounded z-depth-1"
  width="auto"
  height="auto"
  
  
  
  
  
  
  
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
/&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt;</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
&lt;div class="col-sm mt-3 mt-md-0"&gt;
    &lt;figure&gt;
</code></pre></div></div> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/abba/loss_landscape_2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/abba/loss_landscape_2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/abba/loss_landscape_2-1400.webp"></source> <img src="/assets/img/abba/loss_landscape_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <p>&lt;/figure&gt;</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;/div&gt;
</code></pre></div></div> <p>&lt;/div&gt;</p> <div class="caption"> <strong>Left</strong>: Illustration of ABBA‚Äôs parameterization, where the update is expressed as the Hadamard product of two learnable low-rank matrices. <strong>Right</strong>: A toy experiment demonstrating ABBA‚Äôs optimization behavior. We first train a 2-layer MLP to classify the first 8 MNIST digits, then fine-tune it to recognize the last 2. ABBA converges faster and achieves better final performance. </div> <p>‚Äì&gt;</p> <h2 id="practical-stuff-while-implementing-abba">Practical stuff while implementing ABBA</h2> <h3 id="implementing-abba-efficiently">Implementing ABBA efficiently</h3> <div class="theorem"> <strong>Theorem 2.</strong><d-cite key="slyusar1997new"></d-cite> Let $B_1 A_1, B_2 A_2 \in \mathbb{R}^{m \times n}$. Then, $$ (B_1 A_1) \odot (B_2 A_2) = \underbrace{(B_1 \odot_r B_2)}_{m \times r_1 r_2} \underbrace{(A_1^\top \odot_r A_2^\top)^\top}_{r_1 r_2 \times n}, $$ where $\odot_r$ denotes the row-wise Khatri‚ÄìRao product. </div> <p>How does Thm (2) enable efficiency for ABBA? If we used the first form notice we would need to compute $B_1 \odot A_1$ and $B_2 \odot A_2$ i.e. full $m \times n$ which is just as bad as full FT. Thm (2) is basically just the LoRA structure (<em>its not the same space by the way see below</em>). So now we can do $\Delta x = B_{kr}(A_{kr}x)$ where $X_{kr} = (X_1 \odot_r X_2)$.</p> <p>At first glance, applying the Hadamard product directly to two LoRA-style updates ‚Äî like computing \((B_1 A_1) \odot (B_2 A_2)\) ‚Äî seems like a nightmare: you‚Äôd have to materialize full \(m \times n\) matrices, which is as costly as full fine-tuning. That‚Äôs clearly not scalable.</p> <p>But <strong>Theorem 2</strong> gives us a lifeline.</p> <p>It tells us that instead of computing the big matrices first and <em>then</em> applying the Hadamard product, we can rewrite the whole thing using a row-wise Khatri‚ÄìRao product.</p> <p>This looks and feels <em>just like</em> the standard LoRA decomposition ‚Äî a skinny-bottleneck sandwich ‚Äî but with slightly different ingredients. Now we never have to form the full \(m \times n\) matrices explicitly. Instead, we can compute the update as:</p> \[\Delta x = B_{\text{kr}} (A_{\text{kr}} x), \quad \text{where } X_{\text{kr}} = X_1 \odot_r X_2 \in \mathbb{R}^{m \times r_1r_2}\] <p>So we preserve the low-rank efficiency, avoid full matrix computation, and still allow more expressive structure than standard LoRA and HiRA.</p> <blockquote> <p><em>Note: the representational space isn‚Äôt exactly the same as LoRA ‚Äî see the next section ‚Äî but the re-parameterization looks very similar to LoRA.</em></p> </blockquote> <h3 id="abba-space-is-not-the-lora-space">ABBA space is not the LoRA space</h3> <p>A natural question to ask after seeing <strong>Theorem 2</strong> is:<br> why not just apply an SVD-style or LoRA-style decomposition directly on the full Hadamard product and solve for matrices $ A_{\text{kr}} $ and $ B_{\text{kr}} $?</p> <p>In theory, yes ‚Äî you could compute $ A_{\text{kr}} $ and $B_{\text{kr}}$ as if it were just another low-rank matrix approximation. But here‚Äôs the catch:<br> you‚Äôd only recover the <em>combined</em> structure (i.e., the product $B_{\text{kr}} A_{\text{kr}}$), and there‚Äôs no guarantee that this can be cleanly split back into the original four matrices $ A_1, B_1, A_2, B_2 $.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/abba.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Rohit K. Vartak. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>